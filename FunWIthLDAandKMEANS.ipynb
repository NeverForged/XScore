{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17207, 5)\n",
      "(17207, 5)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import gensim\n",
    "import spacy\n",
    "import numpy as np\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "df = pd.read_csv('train.tsv', sep='\\t')\n",
    "print(df.shape)\n",
    "# df = df[df['Score1']==df['Score2']]\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lst_all = []\n",
    "for i in list(set(df['EssaySet'])):\n",
    "    lst_all.append(df[df['EssaySet'] == i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lst_train = []\n",
    "lst_test = []\n",
    "for dfrm in lst_all:\n",
    "    dfrm = dfrm.sample(frac=1)\n",
    "    eighty = int(0.8*dfrm.shape[0])\n",
    "    lst_train.append(dfrm[:eighty])\n",
    "    lst_test.append(dfrm[eighty:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "cvs = []\n",
    "vectors_train = []\n",
    "vectors_test = []\n",
    "for i, dfrm in enumerate(lst_train):\n",
    "    num_docs = lst_train[i].shape[0]\n",
    "    min_df = int(num_docs/500)\n",
    "    cvs.append(CountVectorizer(strip_accents='unicode', \n",
    "                                 ngram_range = (1,7),\n",
    "                                 stop_words='english',\n",
    "                                 min_df = min_df,\n",
    "                                 max_df=0.9,\n",
    "                                 lowercase=True))\n",
    "    cvs[i].fit(dfrm['EssayText'])\n",
    "    vectors_train.append(cvs[i].transform(dfrm['EssayText']))\n",
    "    vectors_test.append(cvs[i].transform(lst_test[i]['EssayText']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "groups_train = []\n",
    "groups_test = []\n",
    "ldas = []\n",
    "kmns = []\n",
    "for i, vector in enumerate(vectors_train):\n",
    "    ldas.append(LatentDirichletAllocation(n_components=900, max_iter=3, n_jobs=1))\n",
    "    kmns.append(KMeans(n_clusters=30))\n",
    "    # append them\n",
    "#     groups_train.append(np.append(vectors_train[i].todense(),ldas[i].fit_transform(vector), axis=1))\n",
    "#     groups_test.append(np.append(vectors_test[i].todense(), ldas[i].transform(vectors_test[i]),axis=1))\n",
    "    # ignore cvs after use\n",
    "#     groups_train.append(ldas[i].fit_transform(vector))\n",
    "#     groups_test.append(ldas[i].transform(vectors_test[i]))\n",
    "    # use ONLY vectors\n",
    "#     groups_train.append(vectors_train[i])\n",
    "#     groups_test.append(vectors_test[i])\n",
    "    # just argmax and the vector\n",
    "#     groups_train.append(np.append(vectors_train[i].todense(), (ldas[i].fit_transform(vector) * np.max(vectors_test[i].todense())).astype(int), axis=1))\n",
    "#     groups_test.append(np.append(vectors_test[i].todense(), (ldas[i].transform(vectors_test[i]) * np.max(vectors_test[i].todense())).astype(int),axis=1))\n",
    "    # vector, lda(vector), kmeans(vecor)\n",
    "#     groups_train.append(np.append(\n",
    "#             np.append(\n",
    "#                 vectors_train[i].todense(),\n",
    "#                 ldas[i].fit_transform(vector), axis=1),\n",
    "#             kmns[i].fit_predict(vectors_train[i]).reshape(-1,1), axis=1))\n",
    "#     groups_test.append(np.append(\n",
    "#             np.append(\n",
    "#                 vectors_test[i].todense(), \n",
    "#                 ldas[i].transform(vectors_test[i]),axis=1),\n",
    "#             kmns[i].predict(vectors_test[i]).reshape(-1,1), axis=1))\n",
    "    \n",
    "#     groups_train.append(np.append(vectors_train[i].todense(),\n",
    "#                                   kmns[i].fit_transform(ldas[i].fit_transform(vector)), axis=1))\n",
    "#     groups_test.append(np.append(vectors_test[i].todense(), \n",
    "#                                  kmns[i].transform(ldas[i].transform(vectors_test[i])),axis=1))\n",
    "    # kmeans of lda of vector\n",
    "#     groups_train.append(kmns[i].fit_transform(ldas[i].fit_transform(vector)))\n",
    "#     groups_test.append(kmns[i].transform(ldas[i].transform(vectors_test[i])))\n",
    "    \n",
    "    # just asint...\n",
    "#     groups_train.append( (ldas[i].fit_transform(vector) * 10).astype(int))\n",
    "#     groups_test.append( (ldas[i].transform(vectors_test[i]) * 10).astype(int))\n",
    "    # kmeans of vector\n",
    "    groups_train.append(kmns[i].fit_transform(vector))\n",
    "    groups_test.append(kmns[i].transform(vectors_test[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (1337, 30) (1337,)\n",
      "1 (1022, 30) (1022,)\n",
      "2 (1512, 30) (1512,)\n",
      "3 (1390, 30) (1390,)\n",
      "4 (1436, 30) (1436,)\n",
      "5 (1437, 30) (1437,)\n",
      "6 (1439, 30) (1439,)\n",
      "7 (1439, 30) (1439,)\n",
      "8 (1438, 30) (1438,)\n",
      "9 (1312, 30) (1312,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "proba1 = []\n",
    "proba2 = []\n",
    "models1 = []\n",
    "models2 = []\n",
    "\n",
    "for i, X in enumerate(groups_train):\n",
    "    print(i,groups_train[i].shape,lst_train[i]['Score1'].shape)\n",
    "    models1.append(GradientBoostingClassifier().fit(groups_train[i],lst_train[i]['Score1']))\n",
    "    proba1.append(models1[i].predict(groups_test[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "Score1 = []\n",
    "Score2 = []\n",
    "\n",
    "for i in range(len(proba1)):\n",
    "    for a in proba1[i]:\n",
    "        Score1.append(a)\n",
    "\n",
    "df_test = pd.DataFrame(columns = lst_test[0].keys())\n",
    "for dfrm in lst_test:\n",
    "    df_test = df_test.append(dfrm)\n",
    "\n",
    "df_test['Score1Pred'] = Score1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall: 0.585 vs 0.869 [0.672]\n",
      "\n",
      "Item 1: 0.409 vs 0.887 [0.461]\n",
      "Item 2: 0.316 vs 0.848 [0.373]\n",
      "Item 3: 0.530 vs 0.755 [0.703]\n",
      "Item 4: 0.563 vs 0.819 [0.688]\n",
      "Item 5: 0.797 vs 0.967 [0.824]\n",
      "Item 6: 0.858 vs 0.967 [0.888]\n",
      "Item 7: 0.506 vs 0.958 [0.528]\n",
      "Item 8: 0.592 vs 0.814 [0.727]\n",
      "Item 9: 0.572 vs 0.806 [0.710]\n",
      "Item 10: 0.619 vs 0.875 [0.707]\n"
     ]
    }
   ],
   "source": [
    "print('overall: {:.3f} vs {:.3f} [{:.3f}]'.format(\n",
    "      df_test[df_test['Score1']==df_test['Score1Pred']].shape[0]/df_test.shape[0],\n",
    "      df_test[df_test['Score1']==df_test['Score2']].shape[0]/df_test.shape[0],\n",
    "       (df_test[df_test['Score1']==df_test['Score1Pred']].shape[0]/df_test.shape[0])/\n",
    "       (df_test[df_test['Score1']==df_test['Score2']].shape[0]/df_test.shape[0])))\n",
    "print()\n",
    "for i in set(df_test['EssaySet']):\n",
    "    df_test1 = df_test[df_test['EssaySet'] == i]\n",
    "    print('Item {}: {:.3f} vs {:.3f} [{:.3f}]'.format(i,\n",
    "          df_test1[df_test1['Score1']==df_test1['Score1Pred']].shape[0]/df_test1.shape[0],\n",
    "          df_test1[df_test1['Score1']==df_test1['Score2']].shape[0]/df_test1.shape[0],\n",
    "          df_test1[df_test1['Score1']==df_test1['Score1Pred']].shape[0]/df_test1[df_test1['Score1']==df_test1['Score2']].shape[0]))                                   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

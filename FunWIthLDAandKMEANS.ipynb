{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import gensim\n",
    "import spacy\n",
    "import numpy as np\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.cluster import KMeans\n",
    "import pickle\n",
    "\n",
    "df = pickle.load( open('processed_df','rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lst_all = []\n",
    "for i in list(set(df['EssaySet'])):\n",
    "    lst_all.append(df[df['EssaySet'] == i])\n",
    "# lst_all.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lst_train = []\n",
    "lst_test = []\n",
    "for dfrm in lst_all:\n",
    "    dfrm = dfrm.sample(frac=1)\n",
    "    eighty = int(0.8*dfrm.shape[0])\n",
    "    lst_train.append(dfrm[:eighty])\n",
    "    lst_test.append(dfrm[eighty:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "cvs = []\n",
    "vectors_train = []\n",
    "vectors_test = []\n",
    "for i, dfrm in enumerate(lst_train):\n",
    "    num_docs = lst_train[i].shape[0]\n",
    "    min_df = int(num_docs/500)\n",
    "    cvs.append(CountVectorizer(strip_accents='unicode', \n",
    "                                 ngram_range = (1,7),\n",
    "                                 stop_words='english',\n",
    "                                 min_df = min_df,\n",
    "                                 max_df=0.9,\n",
    "                                 lowercase=True))\n",
    "    cvs[i].fit(dfrm['EssayPrepped'])\n",
    "    vectors_train.append(cvs[i].transform(dfrm['EssayPrepped']))\n",
    "    vectors_test.append(cvs[i].transform(lst_test[i]['EssayPrepped']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Darin\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\Darin\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\Darin\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\Darin\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\Darin\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\Darin\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\Darin\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\Darin\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\Darin\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\Darin\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "groups_train = []\n",
    "groups_test = []\n",
    "ldas = []\n",
    "kmns = []\n",
    "for i, vector in enumerate(vectors_train):\n",
    "    ldas.append(LatentDirichletAllocation(n_components=90, max_iter=3, n_jobs=1))\n",
    "    kmns.append(KMeans(n_clusters=30))\n",
    "    # append them\n",
    "#     groups_train.append(np.append(vectors_train[i].todense(),ldas[i].fit_transform(vector), axis=1))\n",
    "#     groups_test.append(np.append(vectors_test[i].todense(), ldas[i].transform(vectors_test[i]),axis=1))\n",
    "    # ignore cvs after use\n",
    "#     groups_train.append(ldas[i].fit_transform(vector))\n",
    "#     groups_test.append(ldas[i].transform(vectors_test[i]))\n",
    "    # use ONLY vectors\n",
    "#     groups_train.append(vectors_train[i])\n",
    "#     groups_test.append(vectors_test[i])\n",
    "    # just argmax and the vector\n",
    "#     groups_train.append(np.append(vectors_train[i].todense(), (ldas[i].fit_transform(vector) * np.max(vectors_test[i].todense())).astype(int), axis=1))\n",
    "#     groups_test.append(np.append(vectors_test[i].todense(), (ldas[i].transform(vectors_test[i]) * np.max(vectors_test[i].todense())).astype(int),axis=1))\n",
    "    # vector, lda(vector), kmeans(vecor)\n",
    "    groups_train.append(np.append(\n",
    "            np.append(\n",
    "                vectors_train[i].todense(),\n",
    "                ldas[i].fit_transform(vector), axis=1),\n",
    "            kmns[i].fit_predict(vectors_train[i]).reshape(-1,1), axis=1))\n",
    "    groups_test.append(np.append(\n",
    "            np.append(\n",
    "                vectors_test[i].todense(), \n",
    "                ldas[i].transform(vectors_test[i]),axis=1),\n",
    "            kmns[i].predict(vectors_test[i]).reshape(-1,1), axis=1))\n",
    "    \n",
    "#     groups_train.append(np.append(vectors_train[i].todense(),\n",
    "#                                   kmns[i].fit_transform(ldas[i].fit_transform(vector)), axis=1))\n",
    "#     groups_test.append(np.append(vectors_test[i].todense(), \n",
    "#                                  kmns[i].transform(ldas[i].transform(vectors_test[i])),axis=1))\n",
    "    # kmeans of lda of vector\n",
    "#     groups_train.append(kmns[i].fit_transform(ldas[i].fit_transform(vector)))\n",
    "#     groups_test.append(kmns[i].transform(ldas[i].transform(vectors_test[i])))\n",
    "    \n",
    "    # just asint...\n",
    "#     groups_train.append( (ldas[i].fit_transform(vector) * 10).astype(int))\n",
    "#     groups_test.append( (ldas[i].transform(vectors_test[i]) * 10).astype(int))\n",
    "    # kmeans of vector\n",
    "#     groups_train.append(kmns[i].fit_transform(vector))\n",
    "#     groups_test.append(kmns[i].transform(vectors_test[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (1337, 7500) (1337,)\n",
      "1 (1022, 7812) (1022,)\n",
      "2 (1512, 7446) (1512,)\n",
      "3 (1390, 10921) (1390,)\n",
      "4 (1436, 4069) (1436,)\n",
      "5 (1437, 4277) (1437,)\n",
      "6 (1439, 7263) (1439,)\n",
      "7 (1439, 10439) (1439,)\n",
      "8 (1438, 9126) (1438,)\n",
      "9 (1312, 7745) (1312,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "proba1 = []\n",
    "proba2 = []\n",
    "models1 = []\n",
    "models2 = []\n",
    "\n",
    "for i, X in enumerate(groups_train):\n",
    "    print(i,groups_train[i].shape,lst_train[i]['Score1'].shape)\n",
    "    models1.append(GradientBoostingClassifier().fit(groups_train[i],lst_train[i]['Score1']))\n",
    "    proba1.append(models1[i].predict(groups_test[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "Score1 = []\n",
    "Score2 = []\n",
    "\n",
    "for i in range(len(proba1)):\n",
    "    for a in proba1[i]:\n",
    "        Score1.append(a)\n",
    "\n",
    "df_test = pd.DataFrame(columns = lst_test[0].keys())\n",
    "for dfrm in lst_test:\n",
    "    df_test = df_test.append(dfrm)\n",
    "\n",
    "df_test['Score1Pred'] = Score1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall: 0.687 vs 0.873 [0.787]\n",
      "\n",
      "Item 1: 0.561 vs 0.896 [0.627]\n",
      "Item 2: 0.461 vs 0.891 [0.518]\n",
      "Item 3: 0.528 vs 0.739 [0.714]\n",
      "Item 4: 0.698 vs 0.784 [0.890]\n",
      "Item 5: 0.875 vs 0.972 [0.900]\n",
      "Item 6: 0.875 vs 0.967 [0.905]\n",
      "Item 7: 0.719 vs 0.950 [0.757]\n",
      "Item 8: 0.636 vs 0.819 [0.776]\n",
      "Item 9: 0.706 vs 0.828 [0.852]\n",
      "Item 10: 0.756 vs 0.896 [0.844]\n"
     ]
    }
   ],
   "source": [
    "print('overall: {:.3f} vs {:.3f} [{:.3f}]'.format(\n",
    "      df_test[df_test['Score1']==df_test['Score1Pred']].shape[0]/df_test.shape[0],\n",
    "      df_test[df_test['Score1']==df_test['Score2']].shape[0]/df_test.shape[0],\n",
    "       (df_test[df_test['Score1']==df_test['Score1Pred']].shape[0]/df_test.shape[0])/\n",
    "       (df_test[df_test['Score1']==df_test['Score2']].shape[0]/df_test.shape[0])))\n",
    "print()\n",
    "for i in set(df_test['EssaySet']):\n",
    "    df_test1 = df_test[df_test['EssaySet'] == i]\n",
    "    print('Item {}: {:.3f} vs {:.3f} [{:.3f}]'.format(i,\n",
    "          df_test1[df_test1['Score1']==df_test1['Score1Pred']].shape[0]/df_test1.shape[0],\n",
    "          df_test1[df_test1['Score1']==df_test1['Score2']].shape[0]/df_test1.shape[0],\n",
    "          df_test1[df_test1['Score1']==df_test1['Score1Pred']].shape[0]/df_test1[df_test1['Score1']==df_test1['Score2']].shape[0]))                                   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
